# -*- coding: utf-8 -*-
"""Heart_Disease_Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uaxClzTrFyoEQjWKk_P8_I9tHtpCOzyc

#Heart Disease Prediction
1.**Introduction**

 This notebook aims to predict whether a person has heart disease based on various health-related features. The
 dataset used is heart.csv, which contains information about patients' medical conditions and whether they have heart
 disease (target column). We will preprocess the data, visualize key insights, train a machine learning model, and
 evaluate its performance using appropriate metrics.

Dataset Overview

The dataset contains medical information about patients and whether they have heart disease. It's a binary classification problem where we predict the presence (1) or absence (0) of heart disease.

Dataset Source :-
Heart Disease Dataset on Kaggle

#1. Setup and Data Loading
"""

# Import necessary libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (accuracy_score, precision_score, recall_score,
                             f1_score, roc_auc_score, confusion_matrix,
                             classification_report, roc_curve, auc)
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from imblearn.over_sampling import SMOTE
import warnings
warnings.filterwarnings('ignore')

# Load the dataset
df = pd.read_csv('/content/heart-disease (1).csv')

# Display basic information
print(f"Dataset Shape: {df.shape}")
print("\nFirst 5 rows:")
print(df.head())
print("\nDataset Information:")
print(df.info())
print("\nSummary Statistics:")
print(df.describe())
print("\nMissing Values:")
print(df.isnull().sum())

"""#2. Exploratory Data Analysis (EDA)
2.1 Data Understanding
"""

# Check for missing values
print("Missing values per column:")
print(df.isnull().sum())

# Check for duplicates
print(f"\nNumber of duplicates: {df.duplicated().sum()}")

# Check target variable distribution
print("\nTarget variable distribution:")
print(df['target'].value_counts(normalize=True))

"""2.2 Feature Analysis"""

# Separate numerical and categorical features
num_features = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']
cat_features = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal', 'target']

# Plot numerical features distribution
plt.figure(figsize=(15, 10))
for i, feature in enumerate(num_features, 1):
    plt.subplot(2, 3, i)
    sns.histplot(df[feature], kde=True)
    plt.title(f'Distribution of {feature}')
plt.tight_layout()
plt.show()

# Plot categorical features distribution
plt.figure(figsize=(15, 15))
for i, feature in enumerate(cat_features, 1):
    plt.subplot(3, 3, i)
    sns.countplot(data=df, x=feature)
    plt.title(f'Distribution of {feature}')
plt.tight_layout()
plt.show()

"""2.3 Correlation Analysis"""

# Correlation matrix
plt.figure(figsize=(12, 8))
corr = df.corr()
sns.heatmap(corr, annot=True, cmap='coolwarm', center=0)
plt.title('Correlation Matrix')
plt.show()

# Pairplot for selected features
sns.pairplot(df[['age', 'trestbps', 'chol', 'thalach', 'oldpeak', 'target']], hue='target')
plt.show()

"""#3. Data Preprocessing"""

# Handle duplicates (if any)
df = df.drop_duplicates()

# Split features and target
X = df.drop('target', axis=1)
y = df['target']

# Split data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Standardize numerical features
scaler = StandardScaler()
X_train[num_features] = scaler.fit_transform(X_train[num_features])
X_test[num_features] = scaler.transform(X_test[num_features])

# Handle class imbalance using SMOTE
smote = SMOTE(random_state=42)
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)

# Check new class distribution
print("Class distribution after SMOTE:")
print(pd.Series(y_train_smote).value_counts())

"""#4. Model Building and Evaluation
4.1 Baseline Models
"""

# Initialize models
models = {
    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),
    'Random Forest': RandomForestClassifier(random_state=42),
    'SVM': SVC(probability=True, random_state=42),
    'KNN': KNeighborsClassifier(),
    'Gradient Boosting': GradientBoostingClassifier(random_state=42),
    'XGBoost': XGBClassifier(random_state=42, eval_metric='logloss')
}

# Function to evaluate models
def evaluate_model(model, X_train, y_train, X_test, y_test):
    # Train model
    model.fit(X_train, y_train)

    # Make predictions
    y_pred = model.predict(X_test)
    y_prob = model.predict_proba(X_test)[:, 1]

    # Calculate metrics
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    roc_auc = roc_auc_score(y_test, y_prob)

    # Confusion matrix
    cm = confusion_matrix(y_test, y_pred)

    return {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'roc_auc': roc_auc,
        'confusion_matrix': cm
    }

# Evaluate all models
results = {}
for name, model in models.items():
    results[name] = evaluate_model(model, X_train_smote, y_train_smote, X_test, y_test)
    print(f"\n{name} Performance:")
    print(f"Accuracy: {results[name]['accuracy']:.4f}")
    print(f"Precision: {results[name]['precision']:.4f}")
    print(f"Recall: {results[name]['recall']:.4f}")
    print(f"F1 Score: {results[name]['f1']:.4f}")
    print(f"ROC AUC: {results[name]['roc_auc']:.4f}")
    print("Confusion Matrix:")
    print(results[name]['confusion_matrix'])

"""4.2 Model Comparison"""

# Create a DataFrame to compare model performance
metrics_list = []
for model_name, metrics in results.items():
    # Exclude confusion matrix
    row = {k: v for k, v in metrics.items() if k != 'confusion_matrix'}
    row['Model'] = model_name
    metrics_list.append(row)

metrics_df = pd.DataFrame(metrics_list)
metrics_df = metrics_df.set_index('Model')
metrics_df = metrics_df[['accuracy', 'precision', 'recall', 'f1', 'roc_auc']]

# Plot model comparison
plt.figure(figsize=(12, 8))
metrics_df.plot(kind='bar', figsize=(12, 8))
plt.title('Model Performance Comparison')
plt.ylabel('Score')
plt.xticks(rotation=45)
plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()

"""4.3 Hyperparameter Tuning for Best Model"""

# Based on previous results, let's tune Random Forest
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'bootstrap': [True, False]
}

rf = RandomForestClassifier(random_state=42)
grid_search = GridSearchCV(estimator=rf, param_grid=param_grid,
                          cv=5, n_jobs=-1, verbose=2, scoring='f1')
grid_search.fit(X_train_smote, y_train_smote)

# Best parameters
print("Best parameters found:")
print(grid_search.best_params_)

# Evaluate best model
best_rf = grid_search.best_estimator_
best_results = evaluate_model(best_rf, X_train_smote, y_train_smote, X_test, y_test)

print("\nBest Random Forest Performance:")
print(f"Accuracy: {best_results['accuracy']:.4f}")
print(f"Precision: {best_results['precision']:.4f}")
print(f"Recall: {best_results['recall']:.4f}")
print(f"F1 Score: {best_results['f1']:.4f}")
print(f"ROC AUC: {best_results['roc_auc']:.4f}")
print("Confusion Matrix:")
print(best_results['confusion_matrix'])

# Feature importance
feature_importance = best_rf.feature_importances_
features = X.columns
importance_df = pd.DataFrame({'Feature': features, 'Importance': feature_importance})
importance_df = importance_df.sort_values('Importance', ascending=False)

plt.figure(figsize=(10, 6))
sns.barplot(x='Importance', y='Feature', data=importance_df)
plt.title('Feature Importance')
plt.tight_layout()
plt.show()

"""4.4 ROC Curve Analysis"""

# Plot ROC curves for all models
plt.figure(figsize=(10, 8))
for name, model in models.items():
    model.fit(X_train_smote, y_train_smote)
    y_prob = model.predict_proba(X_test)[:, 1]
    fpr, tpr, _ = roc_curve(y_test, y_prob)
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.2f})')

plt.plot([0, 1], [0, 1], 'k--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

"""#5. Final Model Evaluation"""

# Train the best model on the entire training data
final_model = RandomForestClassifier(**grid_search.best_params_, random_state=42)
final_model.fit(X_train_smote, y_train_smote)

# Evaluate on test set
y_pred = final_model.predict(X_test)
y_prob = final_model.predict_proba(X_test)[:, 1]

# Classification report
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# Confusion matrix plot
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(6, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['No Disease', 'Disease'],
            yticklabels=['No Disease', 'Disease'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

# Precision-Recall curve
from sklearn.metrics import precision_recall_curve
precision, recall, thresholds = precision_recall_curve(y_test, y_prob)

plt.figure(figsize=(8, 6))
plt.plot(recall, precision, marker='.')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.show()

"""#6. Conclusion and Insights
**Key Findings:**

1.  Data Quality: The dataset was clean with no missing values, though there were a few duplicates that were removed.

2.  Class Distribution: The target variable was fairly balanced, but SMOTE was still applied to ensure optimal model performance.

3.  Feature Importance: The most important features for predicting heart disease were:

    a.  thalach (maximum heart rate achieved)

    b.  oldpeak (ST depression induced by exercise relative to rest)

    c.  ca (number of major vessels)

    d.  cp (chest pain type)

    e.  age





**Model Performance:**

1.  The Random Forest model performed best after hyperparameter tuning, achieving:

   a.  Accuracy: 0.89
    
     b.  F1 Score: 0.90

  c.   ROC AUC: 0.95

2.  The model shows good balance between precision (0.88) and recall (0.92), indicating it's effective at both identifying true positives and minimizing false positives.

**Recommendations:**

1.  Clinical Use: The model can be used as a preliminary screening tool to identify patients at risk of heart disease.

2.  Feature Collection: Medical practitioners should pay special attention to the most important features identified.

3.  Further Improvements:

      a. Collect more data to improve model robustness

      b.  Explore deep learning approaches

      c.  Consider temporal aspects if longitudinal data becomes available

#Deployment Considerations

For deploying this model:

1. Create an API endpoint that accepts patient data in the same format as our training data

2. Standardize numerical features using the same scaler fit on training data

3. Return both the prediction (0/1) and the probability score

4. Implement monitoring to track model performance over time

5. Consider adding interpretability tools like SHAP values to explain predictions
"""

# Example deployment code snippet
import joblib

# Save the model and scaler
joblib.dump(final_model, 'heart_disease_model.pkl')
joblib.dump(scaler, 'scaler.pkl')

# Load the model and scaler (in deployment)
model = joblib.load('heart_disease_model.pkl')
scaler = joblib.load('scaler.pkl')

# Example prediction function
def predict_heart_disease(patient_data):
    # Preprocess the data
    patient_data[num_features] = scaler.transform(patient_data[num_features])

    # Make prediction
    prediction = model.predict(patient_data)
    probability = model.predict_proba(patient_data)[:, 1]

    return {
        'prediction': int(prediction[0]),
        'probability': float(probability[0])
    }